---
title: "Publication Bias in Psychology"
subtitle: "Third year paper proposal"
author: "Benjamin Lira"
css: custom.css
# format: html
format: 
  pdf:
    documentclass: article
    classoption: onecolumn
    fontfamily: times
fig-cap-location: top
table-cap-location: bottom
editor: visual
self-contained: true
fontsize: 12pt
# linestretch: 1.6
link-citations: true
keep-tex: false
geometry: margin=1in
header-includes: |
  \usepackage{sectsty}
  \usepackage{helvet}
  \usepackage{xcolor}
  \setlength{\parindent}{1em}
  \setlength{\parskip}{1em}
  \definecolor{darkblue}{RGB}{0,0,139}
  \usepackage{titlesec}
  \allsectionsfont{\sffamily\color{darkblue}}
  \usepackage{etoolbox}
  \usepackage{setspace}
  \patchcmd{\maketitle}{\begingroup}{\begingroup\sffamily\bfseries\color{darkblue}}{}{}
  \titlespacing*{\section}{0pt}{0pt}{10pt}
  \titlespacing*{\subsection}{0pt}{0pt}{5pt}
  \titlespacing*{\subsubsection}{0pt}{0pt}{2pt}
bibliography: ../references.bib
csl: ../apa-numeric-superscript.csl
---

<!-- # Gap this project would fill -->

Effect size is the currency with which psychological research is interpreted. 
However, not all research has the same likelihood of being published.
Studies with large effect sizes and statistically significant results are overrepresented in the scientific literature, leading to a distribution of effect sizes that is biased upward, a phenomenon known as publication bias.
How large is this bias? I propose a meta-review of meta-analyses in psychology to estimate the distribution of true effect sizes across psychology, taking into account bias introduced by publication status.

Specifically, as a first step, I plan to analyze how meta-analysts deal with publication bias.
Second, I plan to estimate the distribution of true effect sizes across psychology, taking into account bias introduced by publication status. 
I plan to estimate the determinants of effect sizes and publication bias, such as **sample size**, **discipline** (i.e., social vs. cognitive psychology), and the **design** of study (i.e., gender differnece, individual difference, or experiment).
Finally, I plan to use the results of the meta-review to conduct a survey of researchers to estimate how miscalibrated they are in judging effect sizes and publication bias. 
Such contribution would allow psychologists to better calibrate their expectations of effect size when planning and reviewing research.

What do we know about effect sizes and publication bias in psychology? First, original published effect sizes are biased upward, and are roughly twice as large as careful replications. The many labs project @opensciencecollaborationEstimatingReproducibilityPsychological2015 showed that the average effect size across 100 published experiments was *r* = .40, whereas the average effect size across 36 replications was *r* = .20. Similarly, evidence from eleven years of student replication projects, also shows that publsied effect sizes (*r* = .30) are roughly twice the size of replications (*r* = .34) @boyceElevenYearsStudent2023. Consistent with this, unpublished effects are, on average .18 SDs smaller than their published counterparts @polaninEstimatingDifferencePublished2016. 

Second, efforts at characterizing median effect sizes have been circumscribed to particular sub-fields. For example, the median effect size has been estimated at *r* = .42 for cognitive neuroscience and psychology @szucsEmpiricalAssessmentPublished2017, at *r* = .19 for individual differences @gignacEffectSizeGuidelines2016, and at *r* = .16 for applied psychology @boscoCorrelationalEffectSize2015. 

Third, evidence suggests that small effect sizes are to be expected even with plenty of data and sophisticated methods. Most notably, Salganik et al @salganik2020 conducted a prediction contest where participants were asked to predict the life outcomes of 1000 people based on a large set of predictors. The best model was only able to explain 20% of the variance in two of the outcomes, and 5% on the other four outcomes. Moreover, The best models barely outperformed a simple linear regression model with only four predictors, suggesting that more data or more predictors is not guarenteed to improve prediction.

Finally, there is a growing consensus that small effect sizes in psychology should be expected and can still be relevant. Ahadi @ahadiMultipleDeterminantsEffect1989 conducted a simulation study that shows that when outcomes are multiply determined (an assumption that likely holds for behavior), the correlation of any one trait predictor is bounded (e.g., at .50 if the behavior is influenced by three traits). Similarly, Funder and Ozer @funder2019 argued that small effects should be interpreted contextually: if a small effect affects a large numbers of people for a long time, the cumulative change could be greater than a more circumscribed but larger effect. Finally, GÃ¶tz @gotzSmallEffectsIndispensable2022 argues that if small effect sizes are the norm rather than the exception, psychology can only be a cumulative science if we adjust our expectations regarding effect size.


<!-- nudges @dellavigna2022 -->

# Proposed research

## Study 1
We survey the meta-analysis literature in psychology in the past 5 years. From that search, we find over 5K published meta-analyses, of which 1.2K have accessible pdfs. 
We can computationally analyze the text in these files to learn how meta-analysts deal with publication bias. 
Preliminary findings, suggest that less than half of meta-analyses conduct an inferential test for publication bias, and fewer than a quarter adjust their findings to account for publication bias.

## Study 2a
We survey pre-registered meta-analyses with existing data posted online. 
When publication status is reported, we compute the empirical distributions of effect size conditional on publication status.  We estimate the file drawer effect size from this. 
Across all datasets, we apply Robust Bayesian Meta-Analysis (RoBMA) @maierRobustBayesianMetaAnalysis2023 to estimate the distribution of true effect sizes.
This allows us to compare the distribution of corrected effect size to that of published effect sizes.
We estimate how moderators of effect size relate to the file drawer effect size and the correction factor.

## Study 2b
A replication of Study 2, but with a different sample of meta-analyses.
We test whether the results of study 2a replicate when estimated from a sample of meta-analyses provided by authors, as opposed to being publicly available.

## Study 3
Given the results of Study 2, we survey researchers who have not conducted meta-analyses to estimate the average correction factor.

# Research questions

-   How do meta-analysts analyze publication bias in the past 5 years of published meta-analyses?
-   What is the distribution of true effect sizes across psychology?
-   How do moderators of effect size relate to the file drawer effect size and the correction factor?
-   How miscalibrated are researchers in judging effect sizes and publication bias?

# Methods

## Study 1

We plan to download all meta-analyses published in the past 5 yeas in psychological journals.
We will then download all available pdfs, and analyze them using natural language processing tools (i.e., dictionary methods, text classifiers) to extract the following information:

1. What percentage of meta-analyses collect unpublished effect sizes?
1. What percentage of meta-analyses conduct an inferential test for publication bias?
2. What percentage of meta-analyses adjust their findings to account for publication bias?
3. How do these trends change over time.

## Study 2

We plan to obtain all meta-analyses with existing data posted online.
From these we can estimate two distribution of effect sizes: one correcting for publication bias, and one uncorrected. From this, I will estimate the average correction factor on published effect sizes. Finally, I plan to use information on the characteristics of the meta-analyses to estimate the determinants of effect sizes and publication bias (i.e., moderation analyses).

## Study 3

We will conduct a survey of researchers to estimate how miscalibrated they are to true effect sizes. Specifically, we will ask them to estimate the average correction factor in ps

<!-- # Progress so far

I have downloaded 5,474 meta-analyses from the past 5 years. Of these, 3,567 have accessible pdfs, from which I am extracting emails. I have created a [survey](https://qfreeaccountssjc1.az1.qualtrics.com/jfe/preview/previewId/cb6934b0-9cf5-4e56-b7ec-b321d1434045/SV_9MQjK2dp8CybTZs?Q_CHL=preview&Q_SurveyVersionID=current) to ask authors to upload their data and categorize their meta-analysis. -->

# Reading List

\linespread{1}
\footnotesize

::: {#refs}
:::
<!-- 
\newpage
\normalsize -->

<!-- # SCRAPS -->

<!-- # Open questions

-   Is this a tractable problem?
-   If it were possible, would it be a useful contribution?
-   We expect to have a missing data problem with some of the moderators of effect size. How big of a problem would this be?
-   Given what already is out there, is it true that this is a case of "go big or go home"?
-   Would it be useful to also use metaBUS for this?

# Notes from Meetings

- Dolores thinks that just using Psych Bulletin data is not gonna fly in Psych Bulletin. It could work for PNAS or some other journal she said.
- An interesting point from Dolores was to use the estimated distribution to try to uncover insights from psychology. To interesting examples: (1) Are effects larger for younger samples? Are kids more malleable? (2) Are effects larger the closer you get to behavior.

Contrary to this, Dalton et al @dalton2012 compared unpublished dissertations with journal articles to argue that there is *no* file drawer inflation bias in perosonnel psychology. -->