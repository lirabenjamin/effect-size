---
title: "Lower your expectations: Effect sizes in psychology"
output: html
bibliography: references.bib
csl: apa-numeric-superscript.csl
---

Live google doc [here](https://docs.google.com/document/d/101GgASJL0igyeBxg-fp-h2wF8i0VCUt8rKH1Po67kcE/edit)

This is a working document on a research project about the true distribution of effect size in psychology and its determinants.

There are three possibilities:

1.  There is a potential empirical contribution, a meta-meta analysis. We would analyze published meta-analyses and examine the distribution of effect sizes and their determinants.
2.  There is a potential contribution of a review/essay/perspective paper. We would synthesize the literature on effect size in psychology and discuss the implications of the findings.
3.  We will conclude that there is no value in pursuing this project.

For now, we will focus on the **empirical** option, which is our working hypothesis.

# Motivation

There is a lot that has been said about effect sizes in psychology. (1) There are a bunch of essays that argue that effect sizes are smaller than Cohen's traditional cutoffs, and are still important (e.g., @g√∂tz2022 @funder2019). (2) There are some meta-analyses for particular subdisciplines, or areas of research (e.g., cognitive neuroscience @szucs2017, nudges @dellavigna2022, applied psychology @bosco2015). (3) Close to what we want to achieve, there is an estimation of the magnitude of the file drawer effect @polanin2016. (4) The many labs replication is also relevant, in that it showed that across 100 published experiments, their average published effect size was *r* = .40, whereas the well-powered replication effect sizes were roughly half of this @opensciencecollaboration2015. (5) There are also empirical studies demonstrating that even with plenty of data and sophisticated methods life outcomes are really hard to predict @salganik2020. (A similar in principle simulation study @ahadi1989 )

Despite all this, there is no definitive answer for a scientist planning a study and trying to answer the question of what effect size should I expect? How should I plan to power my study? Our vision is that our study would be able to model effect sizes such that I could know that the median effect size in psychology is, say, *r* = .15. If I know that we are dealing with a social psychology effect, where the predictor and the outcome do not share common method variance (e.g., open ended text correlating with behavior), and the predictor and outcome are separated by 4 years, then, I can expect my effect size to shrink to *r* = .06.

Aside from the contribution towards sample size planning and power calculations, we also hope to show how miscalibrated researchers are in terms of what effect sizes can be expected, and how we should evaluate them when reading, reviewing, and conducting research.

# Research questions

-   What is the distribution of effect sizes in psychology?
-   How does this distribution change for different types of studies?
    -   Published vs. unpublished
    -   Lab studies vs. field studies
    -   Experimental vs. correlational studies
    -   Social vs. cognitive
    -   Within person vs. between person
    -   Concurrent vs. time-lagged
    -   Common method vs multi-method
-   How miscalibrated are scientists to true effect sizes.

# Approach

-   We will use meta-analyses as our unit of analysis. We will use the meta-analytic effect size as our dependent variable.
-   We could download all meta-analyses published in Psychological Bulletin and code them for the above variables.

# Open questions

-   Is this a tractable problem?
-   If it were possible, would it be a useful contribution?
-   Would it be useful to also use metaBUS for this?
-   We expect to have a missing data problem with some of the moderators of effect size. How big of a problem would this be?
-   Given what already is out there, is it true that this is a case of "go big or go home"?
